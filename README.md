# Evaluating LLMs in Interviews: Key Benchmarks and Model Capabilities
![Image](https://github.com/Omkar1634/Evaluating-LLMs-Interviews/blob/main/webapp/asset/1.png)
## Abstract

The aim of this project is to develop a State-of-the-Art Benchmark for evaluating Large Language Models (LLMs). Models are benchmarked based on their capabilities, such as coding, common sense, and reasoning. Other capabilities encompass natural language processing, including machine translation, question answering, and text summarization. LLM benchmarks play a crucial role in developing and enhancing models. They showcase the progress of an LLM as it learns, with quantitative measures that highlight where the model excels and its areas for improvement. These benchmarks also provide an objective comparison of different models, helping inform software developers and organizations as they choose which models better suit their needs.

## Motivation

In the rapidly evolving field of artificial intelligence, large language models (LLMs) are becoming increasingly integral to various applications, from coding assistance to natural language processing tasks like translation and summarization. As these models grow more sophisticated, it is crucial to have a robust framework for evaluating their capabilities, particularly in high-stakes scenarios such as interviews.

The title "Evaluating LLMs in Interviews: Key Benchmarks and Model Capabilities" reflects the importance of systematic assessment in ensuring that LLMs meet the specific demands of real-world applications. By establishing and utilizing key benchmarks, interviewers can objectively measure the strengths and weaknesses of different models, leading to more informed decisions.

This approach not only helps identify the most suitable models for specific tasks but also drives the continued improvement of LLMs, ensuring that they evolve to meet the ever-growing expectations of users and developers alike. In essence, this project underscores the critical role of benchmarks in the interview process, guiding the selection of LLMs that are not only proficient but also well-matched to the nuanced requirements of diverse applications.

## Guidelines

### How you should frame the question:

- **Focus on Relevant and Specific Content.**
- **Avoid Controversial or Political Topics.**
- **Cultural and Context-Specific Questions.**
- **Incorporate Negations.**
- **Use Complete, Native Language Questions.**
- **Ensure Originality.**
- **Avoid Personal Bias and Subjectivity.**

### Example

![Example Image](https://github.com/Omkar1634/Evaluating-LLMs-Interviews/blob/main/webapp/asset/eg.png)

## Goals and Milestones

### Goals:

- **Develop a state-of-the-art LLMs Benchmark for Interviews.**
- **Develop a Comprehensive Benchmarking Framework.**
- **Establish Objective Evaluation Criteria.**
- **Optimize Interview Processes.**
- **Enhance LLM Capabilities Through Feedback.**

### Milestones:

- **Milestone 1:** Initial research and dataset collection.
- **Milestone 2:** Model benchmarking and preliminary testing.
- **Milestone 3:** Dataset Release and Feedback.
- **Milestone 4:** Final evaluation and publication.

## Contribution

For Contribution, you can click the button below:

link: https://evaluating-llms-interviews.onrender.com/

## Team

### Researcher

**Omkar R Kharkar**  
**Dhanesh Kapadia** <br>
**Vivek Unni**

### Research Advisor

**Academia:**  
**Dr. Katerina Bourazeri**  
_Lecturer, University of Essex_
